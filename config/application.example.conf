ktor {
  application {
    modules = ["net.barrage.llmao.ApplicationKt.module"]
  }
  deployment {
    port = 42069
  }
  environment = "development"
  features {
    whatsApp = true
    specialists {
        jirakira = true
    }
    storage {
      minio = true
    }
    llm {
      azure = true
      openai = true
      ollama = true
      vllm = true
    }
    embeddings {
      azure = true
      openai = true
      fembed = true
    }
  }
}

blob {
    provider = "minio"
    image {
        maxFileSize = 5242880 # 5MB
    }
}

jirakira {
  endpoint = "https://my.jira.endpoint"
}

db {
  url = "jdbc:postgresql://localhost:5454/kappi"
  r2dbcHost = "localhost"
  r2dbcPort = 5454
  r2dbcDatabase = "kappi"
  user = "postgres"
  password = "postgres"
  driver = "org.postgresql.Driver"
  runMigrations = true
}

cors {
  origins = ["http://localhost:3000", "http://localhost:3001"]
  methods = ["GET", "HEAD", "PUT", "PATCH", "POST", "DELETE"]
  headers = []
}

# OpenAI supports dynamically selecting a model on its endpoints, so we only have to put in the model IDs.
# Azure and VLLM select models based on URL path parameters.
# Each key in the `models` property is a deployment
# identifier, and its respective value is the model code used to identify the model.
# Since both use the OpenAI SDK, and the SDK does not support passing a null value for the model,
# each deployment ID (path parameter) must be mapped to a model code (request body parameter).
llm {
  openai {
    endpoint = "https://api.openai.com/v1/"
    apiKey = "apiKey"
    models = ["gpt-4o-mini", "o3-mini", "o1-mini"]
  }
  azure {
    endpoint = "RESOURCE_ID"
    apiVersion = "2023-05-15"
    apiKey = "apiKey"
    models = {
        # Deployment ID = Model Code
        gpt-4o-mini = "gpt-4o-mini"
        o3-mini = "o3-mini"
    }
  }
  ollama {
    endpoint = "endpoint"
  }
  # VLLM is an OpenAI compatible API, follow schema the same way as Azure
  vllm {
    endpoint = "http://192.168.106.28:26669"
    apiKey = "apiKey"
    models = {
       # Deployment ID = Model Code
       qwen = "Qwen/Qwen2.5-1.5B-Instruct" ,
       mistral = "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
    }
  }
}

embeddings {
  openai {
    endpoint = "https://api.openai.com/v1/"
    apiKey = "apiKey"
    models = [
      { model = "text-embedding-ada-002", vectorSize = 1536 },
      { model = "text-embedding-3-small", vectorSize = 1536 },
      { model = "text-embedding-3-large", vectorSize = 3072 },
    ]

  }
  azure {
    endpoint = "RESOURCE_ID"
    apiKey = "apiKey"
    apiVersion = "2023-05-15"
    models = [
      # Deployment ID, Model, Embedding size
      { deploymentId = "text-embedding-ada-002", model = "text-embedding-ada-002", vectorSize = 1536 },
      { deploymentId = "text-embedding-3-small", model = "text-embedding-3-small", vectorSize = 1536 },
      { deploymentId = "text-embedding-3-large", model = "text-embedding-3-large", vectorSize = 3072 },
    ]
  }
  fembed {
    endpoint = "http://192.168.106.28:6969"
  }
}

weaviate {
  host = "localhost:8080"
  scheme = "http"
}

jwt {
    enabled = true
    issuer = "https://authentik.barrage.dev/application/o/ragu/"
    jwksEndpoint = "https://authentik.barrage.dev/application/o/ragu/jwks/"
    audience = "HNANpZlS8W7sF9nAA6uZ65YEPa1tyfWmxvT1rJAP"
    # The claim in the access JWT that holds the user's entitlements (groups).
    entitlementsClaim = "entitlements"

    # The leeway for nbf, iat, and exp claims in seconds.
    leeway = 10
}

infobip {
  apiKey = "apiKey"
  endpoint = "https://RESOURCE.api.infobip.com"
  sender = "sender_number"
  template = "welcome_template_name"
  appName = "app_name"
}

minio {
  endpoint = "http://localhost:9000"
  accessKey = "minioAdmin"
  secretKey = "minioAdmin"
  bucket = "kappi"
}
